# Lesson_9_Malinin_danila
## ГЛАВНОЕ перед запуском: Если не будет работать попробуйте ввести в терминал:
- python -m nltk.downloader stopwords
- python -m nltk.downloader wordnet
- если совсем не работает пишите: d4nilamalinin@yandex.ru
## Вступление
Мне нужно было сделать задачу информационного поиска, то есть создать аналог Яндекса по запросу которого должно выдавать наиболее релевантные сайты (тексты). И я это сделал!
Сначало я использовал dataset с супер героями, состоящий из ~ 1500 героев. Но он весил мало, поэтому я решил взять dataset с краткими книгами - и это была ошибка! Он весил 300 мб и все шло хорошо, но когда нужно было испольозвать матрицу 800 строк на 300 000 столбцов - то было не смешно... Я потратил 3 дня и понял что все, с меня хватит и вернулся к супергероям!
## Ссылки на dataset-ы
- который я исользовал: https://www.kaggle.com/jonathanbesomi/superheroes-nlp-dataset
- который не смог осилить: https://www.kaggle.com/shubchat/1002-short-stories-from-project-guttenberg
## Значения файлов наверху
- server.py - main файл, нужно запустить
- Malinin_Danila_notebook.ipynb - ноутбук как я обрабатывал данные и смотрел метрики качества
- superheroes_nlp_dataset.csv - dataset с данными о супер-героях
## Вывод
Я потратил на это задане не мало времени, и я горжусь что сделал! Теперь вы можете найти вашего супер героя в поисковой строке и "почитать" о его истории
## Из чего состояла моя работа
* Сперва я отредоктировал датасет, отделив title и main content
* Сделал нормализацию + лемитизацию текста для дальнейшей работы
* Сделал словарь 'слово': [список индексов] для того, чтобы исклчать ненужных супергероев, чтобы работало быстрее!
* С помощью tf-idf разложения я преобразовывал входную строку (dataset уже преобразован), и смотрел на релевантность с основным dataset-ом - но как?
* С помощью cosine_similarity смотел метрику схожести с супер-героями (по тексту)
* Выдаю топ 30 релевантных документов для входной строки
* Если нечего не найдено - то выдается один документ "3-D Man with super speed"


 
